{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreianmatos/temporal_spaces_texture_gen/blob/main/styleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDuLAzjizDWc"
      },
      "source": [
        "Use of an open-source PyTorch implementation of StyleGAN2 - https://github.com/lucidrains/stylegan2-pytorch - setup for training using Colab's free GPU resources and Google Drive - https://github.com/96jonesa/StyleGan2-Colab-Demo .\n",
        "\n",
        "Training results and models are saved to the local runtime's 'results' and 'models' directories (folder icon on left bar), or to your Google Drive (give access in cell  below) in subdirectories of a parent directory named 'StyleGan2_small_set_demo'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA1sJxmhzXlh"
      },
      "source": [
        "# StyleGan2:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/andreianmatos/temporal_spaces_texture_gen/blob/main/results/stylegan_gif.gif?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "0. Login to Google (Drive)\n",
        "1. Click 'Copy to Drive' above to make a runnable copy of this notebook.\n",
        "2. Run this cell (click the play button in top left of cell) to connect to a runtime instance.\n",
        "3. Navigate to 'Runtime > Change Runtime Type > Hardware Accelerator' and select GPU.\n",
        "4. If needed, modify the variables found in the cell below to select behavior of demo.\n",
        "5. Run cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8lZLs-H3-_3"
      },
      "outputs": [],
      "source": [
        "USE_DATASET = 'captures'\n",
        "\n",
        "TRAINING_FROM_SCRATCH = True # set True if training from scratch, False if training from last checkpoint\n",
        "MODEL_NAME = 'model'\n",
        "MODEL_NUM_TRAIN_STEPS = 3000\n",
        "\n",
        "LOW_NETWORK_CAPACITY = False # set True to use significantly lower network capacity\n",
        "USE_GOOGLE_DRIVE_FOR_TRAINING = True # save models and results directly to your Google Drive\n",
        "\n",
        "# 'none', 'first', 'every'\n",
        "USE_ATTENTION_LAYERS = 'none' # which layers do you want attention applied to?\n",
        "# dataset consists of relatively simple patterns and structures, and you don't observe significant long-range dependencies in your images\n",
        "\n",
        "MODEL_AUGMENTATION_PROBABILITY = 0\n",
        "MODEL_LEARNING_RATE = 2e-4\n",
        "MODEL_IMAGE_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DBbOtcUlE2w",
        "outputId": "59bce50c-faaf-4d21-c757-58284dadbc7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounts your Google Drive so files can be saved to it. Note that this also allows\n",
        "# files to be read from it, so only authorize this if you are comfortable doing so\n",
        "# and/or using a disposable Google Drive account.\n",
        "\n",
        "if USE_GOOGLE_DRIVE_FOR_TRAINING:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    !mkdir -p \"/content/drive/My Drive/StyleGan2_2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EKhglHWE6bW"
      },
      "outputs": [],
      "source": [
        "MODEL_NETWORK_CAPACITY = 16\n",
        "if LOW_NETWORK_CAPACITY:\n",
        "    MODEL_NETWORK_CAPACITY = 4\n",
        "\n",
        "MODEL_ATTENTION_LAYERS = []\n",
        "if USE_ATTENTION_LAYERS == 'first':\n",
        "    MODEL_ATTENTION_LAYERS = \"[1]\"\n",
        "elif USE_ATTENTION_LAYERS == 'every':\n",
        "    MODEL_ATTENTION_LAYERS = \"[1,2,3,4,5,6]\"\n",
        "\n",
        "MODEL_NAME = USE_DATASET + '_' + MODEL_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ShcNlPdbOnm",
        "outputId": "a452c708-b9f3-4ab9-8c4e-f7545657d984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stylegan2_pytorch==0.17.1\n",
            "  Downloading stylegan2_pytorch-0.17.1-py3-none-any.whl (23 kB)\n",
            "Collecting fire (from stylegan2_pytorch==0.17.1)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stylegan2_pytorch==0.17.1) (1.23.5)\n",
            "Collecting retry (from stylegan2_pytorch==0.17.1)\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stylegan2_pytorch==0.17.1) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from stylegan2_pytorch==0.17.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from stylegan2_pytorch==0.17.1) (0.16.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stylegan2_pytorch==0.17.1) (9.4.0)\n",
            "Collecting adamp (from stylegan2_pytorch==0.17.1)\n",
            "  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting contrastive-learner>=0.1.0 (from stylegan2_pytorch==0.17.1)\n",
            "  Downloading contrastive_learner-0.1.1-py3-none-any.whl (4.9 kB)\n",
            "Collecting linear-attention-transformer (from stylegan2_pytorch==0.17.1)\n",
            "  Downloading linear_attention_transformer-0.19.1-py3-none-any.whl (12 kB)\n",
            "Collecting vector-quantize-pytorch (from stylegan2_pytorch==0.17.1)\n",
            "  Downloading vector_quantize_pytorch-1.12.17-py3-none-any.whl (24 kB)\n",
            "Collecting kornia (from contrastive-learner>=0.1.0->stylegan2_pytorch==0.17.1)\n",
            "  Downloading kornia-0.7.1-py2.py3-none-any.whl (756 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->stylegan2_pytorch==0.17.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->stylegan2_pytorch==0.17.1) (2.4.0)\n",
            "Collecting axial-positional-embedding (from linear-attention-transformer->stylegan2_pytorch==0.17.1)\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops (from linear-attention-transformer->stylegan2_pytorch==0.17.1)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linformer>=0.1.0 (from linear-attention-transformer->stylegan2_pytorch==0.17.1)\n",
            "  Downloading linformer-0.2.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting local-attention (from linear-attention-transformer->stylegan2_pytorch==0.17.1)\n",
            "  Downloading local_attention-1.9.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting product-key-memory>=0.1.5 (from linear-attention-transformer->stylegan2_pytorch==0.17.1)\n",
            "  Downloading product_key_memory-0.2.10-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry->stylegan2_pytorch==0.17.1) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry->stylegan2_pytorch==0.17.1)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->stylegan2_pytorch==0.17.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->stylegan2_pytorch==0.17.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->stylegan2_pytorch==0.17.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->stylegan2_pytorch==0.17.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->stylegan2_pytorch==0.17.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->stylegan2_pytorch==0.17.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->stylegan2_pytorch==0.17.1) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->stylegan2_pytorch==0.17.1) (2.31.0)\n",
            "Collecting einx[torch]>=0.1.3 (from vector-quantize-pytorch->stylegan2_pytorch==0.17.1)\n",
            "  Downloading einx-0.1.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from einx[torch]>=0.1.3->vector-quantize-pytorch->stylegan2_pytorch==0.17.1) (2.4.0)\n",
            "Collecting colt5-attention>=0.10.14 (from product-key-memory>=0.1.5->linear-attention-transformer->stylegan2_pytorch==0.17.1)\n",
            "  Downloading CoLT5_attention-0.10.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->stylegan2_pytorch==0.17.1) (2.1.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia->contrastive-learner>=0.1.0->stylegan2_pytorch==0.17.1) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->stylegan2_pytorch==0.17.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->stylegan2_pytorch==0.17.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->stylegan2_pytorch==0.17.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->stylegan2_pytorch==0.17.1) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->stylegan2_pytorch==0.17.1) (1.3.0)\n",
            "Building wheels for collected packages: adamp, fire, axial-positional-embedding, einx\n",
            "  Building wheel for adamp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=5983 sha256=6cece76208d81ceb59f75d68508e155e0d594e795566b525ad7fb47ac79cc59f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/ad/0f/b41b1c45b18c66e5eef5d2254415af8055c7e2b0934145157d\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=36ba8877ece2dfde8786d26d33ffa99a0b6ac5f796f408be3fd49631c3460244\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2882 sha256=145e2f43bde68c8fecd8aa4982fcd14e668801f8790b7765e89fe7abd6813175\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "  Building wheel for einx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for einx: filename=einx-0.1.3-py3-none-any.whl size=80072 sha256=3d8df2eb02d56fcef009b4793aa0b9a057f65b85445c1fb13605fee62e435b66\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/98/b2/ceed882dc5ccb13727cc2b27bb1d0e504599a2dc679a8f3c4d\n",
            "Successfully built adamp fire axial-positional-embedding einx\n",
            "Installing collected packages: adamp, py, fire, einops, retry, einx, local-attention, linformer, kornia, axial-positional-embedding, vector-quantize-pytorch, contrastive-learner, colt5-attention, product-key-memory, linear-attention-transformer, stylegan2_pytorch\n",
            "Successfully installed adamp-0.3.0 axial-positional-embedding-0.2.1 colt5-attention-0.10.19 contrastive-learner-0.1.1 einops-0.7.0 einx-0.1.3 fire-0.5.0 kornia-0.7.1 linear-attention-transformer-0.19.1 linformer-0.2.3 local-attention-1.9.0 product-key-memory-0.2.10 py-1.11.0 retry-0.9.2 stylegan2_pytorch-0.17.1 vector-quantize-pytorch-1.12.17\n"
          ]
        }
      ],
      "source": [
        "# Installs the architecture from:\n",
        "# https://github.com/lucidrains/stylegan2-pytorch\n",
        "\n",
        "!pip install stylegan2_pytorch==0.17.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv3pbXk1f7f7"
      },
      "outputs": [],
      "source": [
        "# Utilities for downloading publicly shared Google Drive files (from your Google Drive).\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = 'https://docs.google.com/uc?export=download'\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, 'wb') as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUNC4_8K7LWL",
        "outputId": "2714316e-bbea-41a9-c65d-81f0b5ca5208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: linformer in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from linformer) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->linformer) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->linformer) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->linformer) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->linformer) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->linformer) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->linformer) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->linformer) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->linformer) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->linformer) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVkgs8NSgHEo"
      },
      "outputs": [],
      "source": [
        "# Downloads and unzips the selected dataset from your Google Drive.\n",
        "\n",
        "import zipfile\n",
        "\n",
        "if USE_DATASET == 'captures': #'captures'\n",
        "\n",
        "    file_id = '1GcRwMjAZeqKGSngvNoHN0pf5AGDFtvZe'\n",
        "    destination = 'captures.zip'\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "    zip_ref = zipfile.ZipFile('captures.zip', 'r')\n",
        "    zip_ref.extractall('data/captures')\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqsmQTyLan3_"
      },
      "outputs": [],
      "source": [
        "# Chooses the appropriate subdirectory of dataset for training.\n",
        "MODEL_DATA_DIR = 'data/captures/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj5WpwcZRMZ9"
      },
      "outputs": [],
      "source": [
        "# Establish directories for custom models.\n",
        "\n",
        "CUSTOM_RESULTS_DIR = './results'\n",
        "CUSTOM_MODELS_DIR = './models'\n",
        "\n",
        "if USE_GOOGLE_DRIVE_FOR_TRAINING:\n",
        "    CUSTOM_RESULTS_DIR = '\"/content/drive/My Drive/StyleGan2_2/results\"'\n",
        "    CUSTOM_MODELS_DIR = '\"/content/drive/My Drive/StyleGan2_2/models\"'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQUV0N8HQh1-",
        "outputId": "7adc2f84-4ca7-4f43-b636-7837503eedb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captures_model<data/captures/>:   0% 0/3000 [00:00<?, ?it/s]G: -121.57 | D: 241.11 | GP: 143208.06 | PL: 0.01 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:   1% 39/3000 [00:31<38:47,  1.27it/s]G: 4.75 | D: 2.58 | GP: 7137.02 | PL: 0.01 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:   3% 99/3000 [01:15<36:03,  1.34it/s]G: 25.61 | D: 13.02 | GP: 3012.91 | PL: 0.02 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:   5% 143/3000 [01:48<35:14,  1.35it/s]G: 9.63 | D: 2.46 | GP: 949.60 | PL: 0.02 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:   6% 188/3000 [02:21<34:30,  1.36it/s]G: 10.06 | D: 7.82 | GP: 266.40 | PL: 0.03 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:   8% 248/3000 [03:05<33:36,  1.36it/s]G: 1.73 | D: 1.20 | GP: 92.91 | PL: 0.03 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  10% 293/3000 [03:39<33:34,  1.34it/s]G: 1.40 | D: 1.82 | GP: 17.83 | PL: 0.03 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  11% 336/3000 [04:11<32:36,  1.36it/s]G: 1.37 | D: 2.16 | GP: 12.70 | PL: 0.03 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  13% 396/3000 [04:55<32:10,  1.35it/s]G: 1.85 | D: 2.40 | GP: 5.25 | PL: 0.04 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  15% 439/3000 [05:27<31:21,  1.36it/s]G: -0.36 | D: 1.27 | GP: 2.94 | PL: 0.04 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  17% 498/3000 [06:10<30:37,  1.36it/s]G: -0.25 | D: 0.74 | GP: 0.63 | PL: 0.04 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  18% 543/3000 [06:43<29:43,  1.38it/s]G: 1.30 | D: 0.28 | GP: 0.08 | PL: 0.05 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  20% 588/3000 [07:17<29:48,  1.35it/s]G: 1.38 | D: 0.34 | GP: 0.36 | PL: 0.05 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  22% 646/3000 [08:00<29:06,  1.35it/s]G: 2.23 | D: 0.96 | GP: 0.25 | PL: 0.05 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  23% 691/3000 [08:33<28:09,  1.37it/s]G: 1.12 | D: 0.50 | GP: 0.58 | PL: 0.05 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  25% 736/3000 [09:06<27:26,  1.38it/s]G: 0.76 | D: 0.61 | GP: 0.65 | PL: 0.06 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  26% 794/3000 [09:49<26:57,  1.36it/s]G: 0.93 | D: 0.79 | GP: 0.20 | PL: 0.06 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  28% 838/3000 [10:22<26:49,  1.34it/s]G: 2.01 | D: 0.97 | GP: 0.14 | PL: 0.06 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  30% 898/3000 [11:06<26:00,  1.35it/s]G: 2.81 | D: 1.06 | GP: 0.21 | PL: 0.07 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  31% 942/3000 [11:39<25:20,  1.35it/s]G: 1.14 | D: 1.23 | GP: 0.99 | PL: 0.07 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  33% 1000/3000 [12:22<24:45,  1.35it/s]G: 1.52 | D: 0.37 | GP: 0.62 | PL: 0.07 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  35% 1043/3000 [12:54<24:07,  1.35it/s]G: 1.63 | D: 0.63 | GP: 0.13 | PL: 0.07 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  37% 1100/3000 [13:36<23:36,  1.34it/s]G: 2.11 | D: 0.67 | GP: 0.17 | PL: 0.08 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  38% 1143/3000 [14:08<22:46,  1.36it/s]G: 1.48 | D: 0.75 | GP: 0.18 | PL: 0.08 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  40% 1188/3000 [14:42<22:30,  1.34it/s]G: 1.07 | D: 0.50 | GP: 0.59 | PL: 0.08 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  42% 1247/3000 [15:25<21:19,  1.37it/s]G: 0.85 | D: 0.85 | GP: 0.40 | PL: 0.08 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  43% 1292/3000 [15:58<21:04,  1.35it/s]G: 1.88 | D: 0.43 | GP: 0.44 | PL: 0.08 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  45% 1350/3000 [16:42<20:36,  1.33it/s]G: 1.86 | D: 0.48 | GP: 0.18 | PL: 0.09 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  46% 1393/3000 [17:13<19:36,  1.37it/s]G: 0.90 | D: 0.63 | GP: 0.47 | PL: 0.09 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  48% 1437/3000 [17:45<19:08,  1.36it/s]G: 1.38 | D: 0.38 | GP: 0.07 | PL: 0.09 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  50% 1497/3000 [18:29<18:14,  1.37it/s]G: 0.57 | D: 0.44 | GP: 0.40 | PL: 0.09 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  51% 1540/3000 [19:02<18:07,  1.34it/s]G: 1.84 | D: 0.72 | GP: 0.30 | PL: 0.10 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  53% 1599/3000 [19:45<16:56,  1.38it/s]G: 0.95 | D: 0.46 | GP: 0.07 | PL: 0.10 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  55% 1644/3000 [20:19<16:50,  1.34it/s]G: 0.68 | D: 0.57 | GP: 0.24 | PL: 0.10 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  56% 1689/3000 [20:52<15:57,  1.37it/s]G: 1.27 | D: 0.42 | GP: 0.16 | PL: 0.10 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  58% 1748/3000 [21:36<15:26,  1.35it/s]G: 1.78 | D: 0.40 | GP: 0.32 | PL: 0.10 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  60% 1792/3000 [22:08<14:33,  1.38it/s]G: 2.41 | D: 0.41 | GP: 0.08 | PL: 0.10 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  61% 1837/3000 [22:42<14:24,  1.35it/s]G: 1.88 | D: 0.32 | GP: 0.09 | PL: 0.11 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  63% 1897/3000 [23:26<13:37,  1.35it/s]G: 1.72 | D: 0.41 | GP: 0.11 | PL: 0.11 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  65% 1940/3000 [23:57<13:01,  1.36it/s]G: 2.32 | D: 0.41 | GP: 0.12 | PL: 0.11 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  67% 2000/3000 [24:42<12:12,  1.36it/s]G: 2.24 | D: 0.42 | GP: 0.06 | PL: 0.11 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  68% 2044/3000 [25:15<11:43,  1.36it/s]G: 1.15 | D: 0.61 | GP: 0.46 | PL: 0.11 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  70% 2089/3000 [25:48<11:17,  1.34it/s]G: 1.33 | D: 0.17 | GP: 0.08 | PL: 0.11 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  72% 2147/3000 [26:32<10:41,  1.33it/s]G: 2.11 | D: 0.28 | GP: 0.08 | PL: 0.11 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  73% 2191/3000 [27:04<09:59,  1.35it/s]G: 2.25 | D: 0.38 | GP: 0.04 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  75% 2249/3000 [27:47<09:18,  1.34it/s]G: 1.83 | D: 0.23 | GP: 0.13 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  76% 2294/3000 [28:19<08:35,  1.37it/s]G: 0.83 | D: 0.35 | GP: 0.13 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  78% 2337/3000 [28:52<08:15,  1.34it/s]G: 1.88 | D: 0.24 | GP: 0.11 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  80% 2396/3000 [29:35<07:17,  1.38it/s]G: 1.56 | D: 0.36 | GP: 0.08 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  81% 2439/3000 [30:07<06:57,  1.34it/s]G: 1.51 | D: 0.47 | GP: 0.13 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  83% 2499/3000 [30:51<06:09,  1.35it/s]G: 2.03 | D: 0.17 | GP: 0.20 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  85% 2543/3000 [31:24<05:38,  1.35it/s]G: 2.12 | D: 0.26 | GP: 0.08 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  86% 2588/3000 [31:56<04:59,  1.38it/s]G: 1.88 | D: 0.17 | GP: 0.09 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  88% 2645/3000 [32:38<04:20,  1.36it/s]G: 1.30 | D: 0.16 | GP: 0.14 | PL: 0.12 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  90% 2689/3000 [33:11<03:50,  1.35it/s]G: 1.33 | D: 0.04 | GP: 0.12 | PL: 0.13 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  92% 2748/3000 [33:54<03:05,  1.36it/s]G: 1.90 | D: 0.19 | GP: 0.04 | PL: 0.13 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  93% 2792/3000 [34:27<02:34,  1.35it/s]G: 1.67 | D: 0.13 | GP: 0.09 | PL: 0.13 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  95% 2837/3000 [35:10<01:59,  1.37it/s]G: 1.62 | D: 0.15 | GP: 0.16 | PL: 0.13 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  97% 2896/3000 [35:43<01:15,  1.37it/s]G: 2.40 | D: 0.63 | GP: 0.08 | PL: 0.13 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>:  98% 2940/3000 [36:15<00:44,  1.36it/s]G: 1.49 | D: 0.19 | GP: 0.36 | PL: 0.13 | CR: 0.00 | Q: 0.00\n",
            "captures_model<data/captures/>: 100% 3000/3000 [36:59<00:00,  1.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train custom models.\n",
        "\n",
        "if TRAINING_FROM_SCRATCH:\n",
        "    !stylegan2_pytorch --data {MODEL_DATA_DIR} --name {MODEL_NAME} --new --network_capacity {MODEL_NETWORK_CAPACITY} --batch_size 16 \\\n",
        "        --gradient_accumulate_every 4 --num_train_steps {MODEL_NUM_TRAIN_STEPS} --attn_layers {MODEL_ATTENTION_LAYERS} --image_size {MODEL_IMAGE_SIZE} \\\n",
        "        --aug_prob {MODEL_AUGMENTATION_PROBABILITY} --results_dir {CUSTOM_RESULTS_DIR} --models_dir {CUSTOM_MODELS_DIR} --learning_rate {MODEL_LEARNING_RATE}\n",
        "else:\n",
        "    !stylegan2_pytorch --data {MODEL_DATA_DIR} --name {MODEL_NAME} --network_capacity {MODEL_NETWORK_CAPACITY} --batch_size 16 \\\n",
        "        --gradient_accumulate_every 4 --num_train_steps {MODEL_NUM_TRAIN_STEPS} --attn_layers {MODEL_ATTENTION_LAYERS} --image_size {MODEL_IMAGE_SIZE} \\\n",
        "        --aug_prob {MODEL_AUGMENTATION_PROBABILITY} --results_dir {CUSTOM_RESULTS_DIR} --models_dir {CUSTOM_MODELS_DIR} --learning_rate {MODEL_LEARNING_RATE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2KY9zrhNoSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e373a1-4f39-43f7-aafc-19ba581a0bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "continuing from previous epoch - 2\n",
            "100% 100/100 [01:00<00:00,  1.65it/s]\n",
            "interpolation generated at /content/drive/My Drive/StyleGan2_2/results/generated_interpolation/captures_model/generated-02-01-2024_16-07-00\n"
          ]
        }
      ],
      "source": [
        "!stylegan2_pytorch --generate_interpolation --name {MODEL_NAME} --num_image_tiles 16  --image_size {MODEL_IMAGE_SIZE} --results_dir {CUSTOM_RESULTS_DIR + \"/generated_interpolation\"} --models_dir {CUSTOM_MODELS_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsaXkZKaPxc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b3f77d-313b-4448-eb4a-38d90bf50a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-08\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-15\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-20\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-26\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-32\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-37\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-44\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-49\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-09-55\n",
            "continuing from previous epoch - 2\n",
            "sample images generated at /content/drive/My Drive/StyleGan2_2/results/generated_images/captures_model/generated-02-01-2024_16-10-01\n"
          ]
        }
      ],
      "source": [
        "number_images_to_generate = 10\n",
        "for el in range(number_images_to_generate):\n",
        "  !stylegan2_pytorch --generate --name {MODEL_NAME} --num_image_tiles 1  --image_size {MODEL_IMAGE_SIZE} --results_dir {CUSTOM_RESULTS_DIR  + \"/generated_images\"} --models_dir {CUSTOM_MODELS_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNHw8TwH0vGY"
      },
      "source": [
        "## Parameters accepted by model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7SFoPxpcgtU"
      },
      "outputs": [],
      "source": [
        "# parameter                 | default   | description\n",
        "#                           |           |\n",
        "# data                      | ./data    | directory containing data\n",
        "# results_dir               | ./results | directory for checkpoint sample images\n",
        "# models_dir                | ./models  | directory for checkpoint models (saves to and loads from here)\n",
        "# name                      | default   | name to identify model (all outputs will be saved to results_dir/name and models_dir/name)\n",
        "# new                       | False     | if True then starts from scratch, else loads from saved checkpoint model\n",
        "# load_from                 | -1        | if -1 then loads from most recent checkpoint, else loads from checkpoint number load_from\n",
        "# image_size                | 128       | size of (square) images generated and for resizing of data\n",
        "# network_capacity          | 16        | affects number of nodes per layer - decrease to train faster with lower output quality\n",
        "# transparent               | False     | if True then uses RGBA, else uses RGB\n",
        "# batch_size                | 3         | number of images per mini-batch (larger uses more GPU memory)\n",
        "# gradient_accumulate_every | 5         | number of mini-batches to process before optimizing (choice depends on batch_size)\n",
        "# num_train_steps           | 150000    | total steps of forward prop (counting starts from number of steps completed in loaded checkpoint)\n",
        "# learning_rate             | 2e-4      | learning rate\n",
        "# num_workers               | None      | if None then uses as many workers as possible from available CPU cores (for data loading)\n",
        "# save_every                | 1000      | every save_every steps, a checkpoint model and sample images are saved\n",
        "# generate                  | False     | if True then generates sample images from loaded model instead of training\n",
        "# generate_interpolation    | False     | if True then generates .gif interpolation from loaded model instead of training, else does not\n",
        "# num_image_tiles           | 8         | generated samples will be a grid of (num_image_tiles x num_image_tiles) images\n",
        "# trunc_psi                 | 0.75      | affects how far generate images can be from average image (increase for more diversity) w_new = psi * w + (1 - psi) * w_avg\n",
        "# fp16                      | False     | if True then uses fp16 half-precision to lower GPU memory usage (requires apex), else uses full-precision\n",
        "# cl_reg                    | False     | if True then uses contrastive learning on discriminator (possibly improves stability and quality), else does not\n",
        "# fq_layers                 | []        | list of layers to apply feature (intermediate representation) vector quantization to (can improve results, but not dramatically)\n",
        "# fq_dict_size              | 256       | dictionary size for feature quantization\n",
        "# attn_layers               | []        | list of layers to apply self-attention to while training (can be empty; do not use spaces; up to log2(image_size) - 1 layers)\n",
        "# no_const                  | False     | if True then 4x4 block is learned from style vector, else styles a constant learned 4x4 block through progressive upsampling\n",
        "# aug_prob                  | 0.0       | probability of applying differentiable augmentation to images fed to discriminator"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzdoqyfKwxRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}